{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNbxxRXTCak"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam,Nadam, SGD\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSF1zp2ZYiB_"
      },
      "outputs": [],
      "source": [
        "bag_of_words = []\n",
        "test_data = []\n",
        "with open('/content/drive/MyDrive/Data/train-data.dat') as data:\n",
        "    for line in data:\n",
        "        bag = [0 for _ in range(8520)]\n",
        "        inline = line.strip().split()\n",
        "        for index, value in enumerate(inline):\n",
        "            if value[0] != '<':\n",
        "                bag[int(value)] += 1\n",
        "        bag_of_words.append(bag)\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/Data/test-data.dat') as data:\n",
        "    for line in data:\n",
        "        bag = [0 for _ in range(8520)]\n",
        "        inline = line.strip().split()\n",
        "        for index, value in enumerate(inline):\n",
        "            if value[0] != '<':\n",
        "                bag[int(value)] += 1\n",
        "        test_data.append(bag)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj6jzAVOZG9P"
      },
      "outputs": [],
      "source": [
        "# parsing labels.txt into a list\n",
        "with open('/content/drive/MyDrive/Data/labels.txt') as data:\n",
        "    genre = []\n",
        "    for line in data:\n",
        "        inline = line.strip().split(',')\n",
        "        genre.append(inline[0])\n",
        "\n",
        "# parsing train-label.dat into a list\n",
        "with open(\"/content/drive/MyDrive/Data/train-label.dat\") as data:\n",
        "    labels = []\n",
        "    for line in data:\n",
        "        labels.append([int(i) for i in line.strip().split()])\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Data/test-label.dat\") as data:\n",
        "    test_labels = []\n",
        "    for line in data:\n",
        "        test_labels.append([int(i) for i in line.strip().split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj90NOlTZQPr"
      },
      "outputs": [],
      "source": [
        "def normalization(list_of_lists):\n",
        "    normalized = []\n",
        "    for lista in list_of_lists:\n",
        "        minimum_number = min(lista)\n",
        "        maximum_number = max(lista)\n",
        "        normalized_bag = [(i - minimum_number) / (maximum_number - minimum_number) for i in lista]\n",
        "        normalized.append(normalized_bag)\n",
        "    return normalized\n",
        "\n",
        "normalized_data = normalization(bag_of_words)\n",
        "normalized_test_data = normalization(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u1rjR-XZY--"
      },
      "outputs": [],
      "source": [
        "X = np.array(normalized_data)\n",
        "Y = np.array(labels)\n",
        "xx_test = np.array(normalized_test_data)\n",
        "yy_test = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGg4TaioZbWF"
      },
      "outputs": [],
      "source": [
        "def get_model(n_inputs, n_outputs):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense((n_outputs+n_inputs)/2, input_dim = n_inputs, kernel_regularizer=l2(0.9), kernel_initializer='he_uniform', activation='sigmoid'))\n",
        "    model.add(Dense((n_outputs), kernel_initializer='he_uniform', activation='sigmoid'))\n",
        "\n",
        "    model.add(Dense(n_outputs, activation='linear'))\n",
        "\n",
        "    \n",
        "    sgd = SGD(learning_rate = 0.001,momentum=0.6)\n",
        "  \n",
        "    model.compile(loss='mse', optimizer= sgd , metrics = ['binary_accuracy','binary_crossentropy','mse'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(X, y):\n",
        "    results = []\n",
        "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "    \n",
        "    best_acc = 0\n",
        "\n",
        "    # k-Fold\n",
        "    cv = KFold(n_splits = 5)\n",
        "    for i, (train_ix, test_ix) in enumerate(cv.split(X)):\n",
        "        \n",
        "        X_train, X_test = X[train_ix], X[test_ix]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        \n",
        "        model = get_model(n_inputs, n_outputs)\n",
        "\n",
        "        #early stopping\n",
        "        early_stopping = tensorflow.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"loss\",restore_best_weights=True,mode='min',patience = 10,min_delta =0.01,verbose=1)\n",
        "\n",
        "        \n",
        "        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs= 100,batch_size=150,verbose=0,callbacks = [early_stopping])\n",
        "\n",
        "\n",
        "        plt.plot(history.history['loss'], label='train')\n",
        "        plt.plot(history.history['val_loss'], label='test')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "        loss, acc, bce, mse = model.evaluate(X_test, y_test)\n",
        "        results.append(acc)\n",
        "\n",
        "\n",
        "        #keep the best performing network\n",
        "        if acc > best_acc:\n",
        "          indx = i+1\n",
        "          best_model = model\n",
        "          best_history = history\n",
        "          best_acc = acc\n",
        "\n",
        "    #plot loss of the training data\n",
        "    plt.plot(best_history.history['loss'], label='train')\n",
        "    plt.plot(best_history.history['val_loss'], label='test')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    #best performing network on test data\n",
        "    test_model = best_model.evaluate(xx_test,yy_test)\n",
        "    print(\"--------------------------TEST--------------------------\")\n",
        "    print(f'Binary Accuracy: {test_model[1]:.4f} | Binary CE: {test_model[2]:.4f} | MSE: {test_model[3]:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "# evaluate model\n",
        "evaluate_model(X, Y)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Neural Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}